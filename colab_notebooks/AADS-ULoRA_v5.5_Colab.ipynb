{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AADS-ULoRA v5.5 - Google Colab Notebook\n",
    "\n",
    "This notebook provides a complete setup for running the AADS-ULoRA v5.5 project on Google Colab with GPU support.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload this notebook to Google Colab\n",
    "2. Change runtime type to GPU (Runtime â†’ Change runtime type â†’ GPU)\n",
    "3. Run all cells sequentially\n",
    "4. Mount your Google Drive to persist data between sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  Warning: No GPU detected. Training will be very slow.\")\n",
    "    print(\"Go to Runtime â†’ Change runtime type â†’ Select GPU\")\n",
    "    print(\"\\nRecommended: Select A100 GPU for best performance (if available)\")\n",
    "    print(\"A100 provides ~50GB GPU memory vs T4's ~15GB\")\n",
    "    print(\"If A100 not available, T4 can still work with smaller batch sizes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nGoogle Drive mounted at /content/drive\")\n",
    "print(\"\\nCreate a project directory in your Drive, e.g.:\")\n",
    "print(\"  /content/drive/MyDrive/aads-ulora-v5.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone-repo"
   },
   "outputs": [],
   "source": [
    "# Clone the project from GitHub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define project directory\n",
    "PROJECT_DIR = '/content/drive/MyDrive/bitirmeprojesi'\n",
    "LOCAL_DIR = '/content/bitirmeprojesi'\n",
    "\n",
    "# Clone from GitHub\n",
    "print(\"Cloning AADS-ULoRA v5.5 from GitHub...\")\n",
    "!git clone https://github.com/EfeErim/bitirmeprojesi.git {LOCAL_DIR}\n",
    "\n",
    "# Copy to Google Drive for persistence (optional)\n",
    "if os.path.exists('/content/drive'):\n",
    "    os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "    shutil.copytree(LOCAL_DIR, PROJECT_DIR, dirs_exist_ok=True)\n",
    "    print(f\"âœ… Project cloned to: {LOCAL_DIR}\")\n",
    "    print(f\"âœ… Project also saved to Drive: {PROJECT_DIR}\")\n",
    "    PROJECT_DIR = LOCAL_DIR  # Use local copy for faster access\n",
    "else:\n",
    "    PROJECT_DIR = LOCAL_DIR\n",
    "    print(f\"âœ… Project cloned to: {LOCAL_DIR}\")\n",
    "\n",
    "print(\"\\nProject structure:\")\n",
    "!ls -la {PROJECT_DIR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev\n",
    "\n",
    "# Set project directory (from previous cell)\n",
    "if 'PROJECT_DIR' not in locals():\n",
    "    PROJECT_DIR = '/content/bitirmeprojesi'  # Default from clone\n",
    "\n",
    "os.chdir(PROJECT_DIR)\n",
    "print(f\"Working in: {PROJECT_DIR}\")\n",
    "\n",
    "# Install Python dependencies\n",
    "print(\"\\nInstalling requirements...\")\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install the package in development mode\n",
    "print(\"\\nInstalling AADS-ULoRA package...\")\n",
    "!pip install -q -e .\n",
    "\n",
    "print(\"\\nâœ… Installation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-install"
   },
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check if src module is importable\n",
    "try:\n",
    "    import src\n",
    "    print(\"âœ… src module imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Failed to import src: {e}\")\n",
    "\n",
    "# List project structure\n",
    "print(\"\\nProject structure:\")\n",
    "!ls -la\n",
    "\n",
    "# Check data directory\n",
    "if os.path.exists('data'):\n",
    "    print(\"\\nData directory contents:\")\n",
    "    !ls -la data/\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No data directory found. You'll need to upload your dataset.\")\n",
    "    print(\"Upload data to: ./data/\")\n",
    "    print(\"Expected structure: data/tomato/, data/pepper/, data/corn/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-upload"
   },
   "source": [
    "## Data Upload\n",
    "\n",
    "You have several options to get your data into Colab:\n",
    "\n",
    "### Option 1: Upload from local computer\n",
    "Use the file browser on the left to upload your `data/` folder\n",
    "\n",
    "### Option 2: Mount Google Drive and copy\n",
    "If your data is already in Google Drive:\n",
    "```python\n",
    "shutil.copytree('/content/drive/MyDrive/path/to/data', './data', dirs_exist_ok=True)\n",
    "```\n",
    "\n",
    "### Option 3: Download from cloud storage\n",
    "Use `!wget` or `!gdown` to download from URLs\n",
    "\n",
    "### Expected data structure:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ tomato/\n",
    "â”‚   â”œâ”€â”€ phase1/\n",
    "â”‚   â”‚   â”œâ”€â”€ healthy/\n",
    "â”‚   â”‚   â”œâ”€â”€ early_blight/\n",
    "â”‚   â”‚   â””â”€â”€ ...\n",
    "â”‚   â”œâ”€â”€ val/\n",
    "â”‚   â””â”€â”€ test/\n",
    "â”œâ”€â”€ pepper/\n",
    "â””â”€â”€ corn/\n",
    "```"
   ]
  },
  {
    "cell_type": "markdown",
    "metadata": {
      "id": "kaggle-dataset"
    },
    "source": [
      "## Download Tomato Disease Dataset from Kaggle\n",
      "\n",
      "This will download the [Tomato Disease Multiple Sources](https://www.kaggle.com/datasets/cookiefinder/tomato-disease-multiple-sources) dataset and organize it for training.\n",
      "\n",
      "### Setup Kaggle API\n",
      "1. Go to Kaggle â†’ Account â†’ API â†’ Create API token\n",
      "2. Upload `kaggle.json` to Colab when prompted\n",
      "3. The dataset will be downloaded and automatically organized into the required structure"
    ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
      "id": "setup-kaggle"
    },
    "outputs": [],
    "source": [
      "# Install Kaggle API\n",
      "!pip install -q kaggle\n",
      "\n",
      "# Upload Kaggle API credentials\n",
      "from google.colab import files\n",
      "import os\n",
      "\n",
      "print(\"Please upload your kaggle.json API credential file\")\n",
      "uploaded = files.upload()\n",
      "\n",
      "if 'kaggle.json' in uploaded:\n",
      "    # Move to .kaggle directory\n",
      "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
      "    shutil.move('kaggle.json', os.path.expanduser('~/.kaggle/kaggle.json'))\n",
      "    os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
      "    print(\"âœ… Kaggle API credentials configured\")\n",
      "else:\n",
      "    print(\"âš ï¸  kaggle.json not found. Please upload it to continue.\")\n",
      "    print(\"Get it from: https://www.kaggle.com/account\")\n"
     ]
    },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
      "id": "download-dataset"
    },
    "outputs": [],
    "source": [
      "# Download the tomato disease dataset from Kaggle\n",
      "print(\"Downloading tomato disease dataset from Kaggle...\")\n",
      "print(\"This may take a few minutes depending on dataset size.\")\n",
      "\n",
      "# Create data directory\n",
      "os.makedirs('./data', exist_ok=True)\n",
      "\n",
      "# Download dataset\n",
      "!kaggle datasets download -d cookiefinder/tomato-disease-multiple-sources -p ./data --unzip\n",
      "\n",
      "print(\"\\nâœ… Dataset downloaded and extracted!\")\n",
      "print(\"\\nChecking extracted files...\")\n",
      "!ls -la ./data/\n"
     ]
    },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
      "id": "organize-dataset"
    },
    "outputs": [],
    "source": [
      "# Organize dataset into AADS-ULoRA expected structure\n",
      "import random\n",
      "from pathlib import Path\n",
      "\n",
      "def organize_tomato_dataset(source_dir: str, target_dir: str):\n",
      "    \"\"\"\n",
      "    Organize the Kaggle tomato dataset into AADS-ULoRA structure.\n",
      "    \n",
      "    Expected structure after organization:\n",
      "    data/tomato/\n",
      "    â”œâ”€â”€ phase1/  (training data)\n",
      "    â”‚   â”œâ”€â”€ healthy/\n",
      "    â”‚   â”œâ”€â”€ bacterial_spot/\n",
      "    â”‚   â”œâ”€â”€ early_blight/\n",
      "    â”‚   â”œâ”€â”€ late_blight/\n",
      "    â”‚   â”œâ”€â”€ leaf_curl/\n",
      "    â”‚   â”œâ”€â”€ leaf_mold/\n",
      "    â”‚   â”œâ”€â”€ septoria_leaf_spot/\n",
      "    â”‚   â”œâ”€â”€ spider_mites/\n",
      "    â”‚   â”œâ”€â”€ target_spot/\n",
      "    â”‚   â”œâ”€â”€ mosaic_virus/\n",
      "    â”‚   â”œâ”€â”€ yellow_leaf_curl_virus/\n",
      "    â”‚   â””â”€â”€ ...\n",
      "    â”œâ”€â”€ val/  (validation data)\n",
      "    â””â”€â”€ test/  (test data)\n",
      "    \"\"\"\n",
      "    source = Path(source_dir)\n",
      "    target = Path(target_dir)\n",
      "    \n",
      "    # Create target directories\n",
      "    (target / 'tomato' / 'phase1').mkdir(parents=True, exist_ok=True)\n",
      "    (target / 'tomato' / 'val').mkdir(parents=True, exist_ok=True)\n",
      "    (target / 'tomato' / 'test').mkdir(parents=True, exist_ok=True)\n",
      "    \n",
      "    # The Kaggle dataset structure may vary. Let's detect it.\n",
      "    print(\"Detecting dataset structure...\")\n",
      "    \n",
      "    # Check common structures\n",
      "    possible_structures = [\n",
      "        source / 'train',\n",
      "        source / 'valid',\n",
      "        source / 'test',\n",
      "        source / 'Train',\n",
      "        source / 'Validation',\n",
      "        source / 'Test'\n",
      "    ]\n",
      "    \n",
      "    # Find the main data directories\n",
      "    train_dir = None\n",
      "    val_dir = None\n",
      "    test_dir = None\n",
      "    \n",
      "    for p in possible_structures:\n",
      "        if p.exists() and p.is_dir():\n",
      "            if 'train' in p.name.lower():\n",
      "                train_dir = p\n",
      "            elif 'valid' in p.name.lower() or 'val' in p.name.lower():\n",
      "                val_dir = p\n",
      "            elif 'test' in p.name.lower():\n",
      "                test_dir = p\n",
      "    \n",
      "    # If not found, check if source directly contains class folders\n",
      "    if train_dir is None:\n",
      "        # Assume the source directory itself contains class folders\n",
      "        train_dir = source\n",
      "        val_dir = None\n",
      "        test_dir = None\n",
      "    \n",
      "    print(f\"Detected structure:\")\n",
      "    print(f\"  Train: {train_dir}\")\n",
      "    print(f\"  Val: {val_dir}\")\n",
      "    print(f\"  Test: {test_dir}\")\n",
      "    \n",
      "    # Get class names from train directory\n",
      "    class_folders = [d for d in train_dir.iterdir() if d.is_dir()]\n",
      "    class_names = [d.name for d in class_folders]\n",
      "    print(f\"\\nDetected classes: {class_names}\")\n",
      "    \n",
      "    # Copy/ organize files\n",
      "    tomato_target = target / 'tomato'\n",
      "    \n",
      "    # Organize training data\n",
      "    print(\"\\nOrganizing training data...\")\n",
      "    for class_name in class_names:\n",
      "        source_class_dir = train_dir / class_name\n",
      "        if not source_class_dir.exists():\n",
      "            continue\n",
      "            \n",
      "        target_class_dir = tomato_target / 'phase1' / class_name\n",
      "        target_class_dir.mkdir(parents=True, exist_ok=True)\n",
      "        \n",
      "        # Copy all images\n",
      "        image_files = list(source_class_dir.glob('*.jpg')) + \\\n",
      "                     list(source_class_dir.glob('*.jpeg')) + \\\n",
      "                     list(source_class_dir.glob('*.png'))\n",
      "        \n",
      "        for img_file in image_files:\n",
      "            shutil.copy2(img_file, target_class_dir / img_file.name)\n",
      "        \n",
      "        print(f\"  {class_name}: {len(image_files)} images\")\n",
      "    \n",
      "    # Organize validation data (if exists)\n",
      "    if val_dir and val_dir.exists():\n",
      "        print(\"\\nOrganizing validation data...\")\n",
      "        for class_name in class_names:\n",
      "            source_class_dir = val_dir / class_name\n",
      "            if not source_class_dir.exists():\n",
      "                continue\n",
      "                \n",
      "            target_class_dir = tomato_target / 'val' / class_name\n",
      "            target_class_dir.mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            image_files = list(source_class_dir.glob('*.jpg')) + \\\n",
      "                         list(source_class_dir.glob('*.jpeg')) + \\\n",
      "                         list(source_class_dir.glob('*.png'))\n",
      "            \n",
      "            for img_file in image_files:\n",
      "                shutil.copy2(img_file, target_class_dir / img_file.name)\n",
      "            \n",
      "            print(f\"  {class_name}: {len(image_files)} images\")\n",
      "    else:\n",
      "        # Split training data into train/val (80/20)\n",
      "        print(\"\\nNo validation set found. Splitting training data (80/20)...\")\n",
      "        for class_name in class_names:\n",
      "            source_class_dir = tomato_target / 'phase1' / class_name\n",
      "            if not source_class_dir.exists():\n",
      "                continue\n",
      "                \n",
      "            images = list(source_class_dir.glob('*.jpg')) + \\\n",
      "                    list(source_class_dir.glob('*.jpeg')) + \\\n",
      "                    list(source_class_dir.glob('*.png'))\n",
      "            \n",
      "            random.shuffle(images)\n",
      "            split_idx = int(len(images) * 0.8)\n",
      "            val_images = images[split_idx:]\n",
      "            \n",
      "            target_val_dir = tomato_target / 'val' / class_name\n",
      "            target_val_dir.mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            for img_file in val_images:\n",
      "                shutil.move(str(img_file), str(target_val_dir / img_file.name))\n",
      "            \n",
      "            print(f\"  {class_name}: moved {len(val_images)} images to validation\")\n",
      "    \n",
      "    # Organize test data (if exists)\n",
      "    if test_dir and test_dir.exists():\n",
      "        print(\"\\nOrganizing test data...\")\n",
      "        for class_name in class_names:\n",
      "            source_class_dir = test_dir / class_name\n",
      "            if not source_class_dir.exists():\n",
      "                continue\n",
      "                \n",
      "            target_class_dir = tomato_target / 'test' / class_name\n",
      "            target_class_dir.mkdir(parents=True, exist_ok=True)\n",
      "            \n",
      "            image_files = list(source_class_dir.glob('*.jpg')) + \\\n",
      "                         list(source_class_dir.glob('*.jpeg')) + \\\n",
      "                         list(source_class_dir.glob('*.png'))\n",
      "            \n",
      "            for img_file in image_files:\n",
      "                shutil.copy2(img_file, target_class_dir / img_file.name)\n",
      "            \n",
      "            print(f\"  {class_name}: {len(image_files)} images\")\n",
      "    else:\n",
      "        # If no test set, use a portion of training as test\n",
      "        print(\"\\nNo test set found. Creating from training data (10%)...\")\n",
      "        for class_name in class_names:\n",
      "            source_class_dir = tomato_target / 'phase1' / class_name\n",
      "            if not source_class_dir.exists():\n",
      "                continue\n",
      "                \n",
      "            images = list(source_class_dir.glob('*.jpg')) + \\\n",
      "                    list(source_class_dir.glob('*.jpeg')) + \\\n",
      "                    list(source_class_dir.glob('*.png'))\n",
      "            \n",
      "            if len(images) > 10:\n",
      "                test_count = max(1, int(len(images) * 0.1))\n",
      "                test_images = images[-test_count:]\n",
      "                \n",
      "                target_test_dir = tomato_target / 'test' / class_name\n",
      "                target_test_dir.mkdir(parents=True, exist_ok=True)\n",
      "                \n",
      "                for img_file in test_images:\n",
      "                    shutil.move(str(img_file), str(target_test_dir / img_file.name))\n",
      "                \n",
      "                print(f\"  {class_name}: moved {len(test_images)} images to test\")\n",
      "    \n",
      "    print(\"\\nâœ… Dataset organization complete!\")\n",
      "    print(f\"\\nFinal structure: {tomato_target}\")\n",
      "    print(\"\\nClass distribution:\")\n",
      "    for split in ['phase1', 'val', 'test']:\n",
      "        split_dir = tomato_target / split\n",
      "        if split_dir.exists():\n",
      "            print(f\"  {split}:\")\n",
      "            for class_dir in sorted(split_dir.iterdir()):\n",
      "                if class_dir.is_dir():\n",
      "                    count = len(list(class_dir.glob('*.*')))\n",
      "                    print(f\"    {class_dir.name}: {count} images\")\n",
      "\n",
      "# Run organization\n",
      "organize_tomato_dataset('./data', './data')\n",
      "\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "print(\"Dataset is ready for training!\")\n",
      "print(\"=\"*50)\n"
     ]
    },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training-section"
   },
   "source": [
    "## Training Pipeline\n",
    "\n",
    "The AADS-ULoRA v5.5 system uses a three-phase training pipeline:\n",
    "\n",
    "1. **Phase 1 - DoRA**: Base adapter training\n",
    "2. **Phase 2 - SD-LoRA**: Continual Instance Learning\n",
    "3. **Phase 3 - CONEC-LoRA**: Contrastive Learning for OOD detection\n",
    "\n",
    "You can also train individual components as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-router"
   },
   "outputs": [],
   "source": [
    "# Train the Crop Router (L1)\n",
    "print(\"Starting Crop Router training...\")\n",
    "!python -m src.router.simple_crop_router \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --output_dir ./outputs/router \\\n",
    "  --epochs 10 \\\n",
    "  --batch_size 32 \\\n",
    "  --lr 1e-4 \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Router training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-phase1"
   },
   "outputs": [],
   "source": [
    "# Phase 1: DoRA Training\n",
    "print(\"Starting Phase 1 - DoRA training...\")\n",
    "print(\"A100 GPU: Using batch_size=32 for optimal performance\")\n",
    "!python -m src.training.phase1_training \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --output_dir ./outputs/tomato_phase1 \\\n",
    "  --batch_size 32 \\\n",
    "  --epochs 50 \\\n",
    "  --lr 1e-4 \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Phase 1 complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-phase2"
   },
   "outputs": [],
   "source": [
    "# Phase 2: SD-LoRA Training\n",
    "print(\"Starting Phase 2 - SD-LoRA training...\")\n",
    "print(\"A100 GPU: Using batch_size=32 for optimal performance\")\n",
    "!python -m src.training.phase2_sd_lora \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --checkpoint ./outputs/tomato_phase1/checkpoints/best_model.pt \\\n",
    "  --output_dir ./outputs/tomato_phase2 \\\n",
    "  --batch_size 32 \\\n",
    "  --epochs 30 \\\n",
    "  --lr 5e-5 \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Phase 2 complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-phase3"
   },
   "outputs": [],
   "source": [
    "# Phase 3: CONEC-LoRA Training\n",
    "print(\"Starting Phase 3 - CONEC-LoRA training...\")\n",
    "print(\"A100 GPU: Using batch_size=32 for optimal performance\")\n",
    "!python -m src.training.phase3_conec_lora \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --checkpoint ./outputs/tomato_phase2/checkpoints/best_model.pt \\\n",
    "  --output_dir ./outputs/tomato_phase3 \\\n",
    "  --batch_size 32 \\\n",
    "  --epochs 20 \\\n",
    "  --lr 1e-5 \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Phase 3 complete!\")\n",
    "print(\"\\nðŸŽ‰ Full training pipeline complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## Inference & Evaluation\n",
    "\n",
    "Run inference on test data and evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate the trained model\n",
    "print(\"Running evaluation...\")\n",
    "!python -m src.evaluation.metrics \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --checkpoint ./outputs/tomato_phase3/checkpoints/best_model.pt \\\n",
    "  --output_dir ./outputs/evaluation \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Evaluation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualization"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Generate visualizations of results, OOD detection, and training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Generate visualizations\n",
    "print(\"Generating visualizations...\")\n",
    "!python -m src.visualization.visualization \\\n",
    "  --data_dir ./data \\\n",
    "  --crop tomato \\\n",
    "  --checkpoint ./outputs/tomato_phase3/checkpoints/best_model.pt \\\n",
    "  --output_dir ./outputs/visualizations \\\n",
    "  --device cuda\n",
    "\n",
    "print(\"\\nâœ… Visualizations generated!\")\n",
    "\n",
    "# Display visualizations\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "vis_dir = './outputs/visualizations'\n",
    "if os.path.exists(vis_dir):\n",
    "    print(\"\\nGenerated plots:\")\n",
    "    for file in os.listdir(vis_dir):\n",
    "        if file.endswith('.png') or file.endswith('.jpg'):\n",
    "            print(f\"  - {file}\")\n",
    "            display(Image(filename=os.path.join(vis_dir, file)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api"
   },
   "source": [
    "## API Server\n",
    "\n",
    "Start the FastAPI server for making predictions via REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-api"
   },
   "outputs": [],
   "source": [
    "# Start the API server in the background\n",
    "!pip install -q uvicorn[standard]\n",
    "\n",
    "print(\"Starting API server...\")\n",
    "print(\"Server will be available at: http://localhost:8000\")\n",
    "print(\"API documentation at: http://localhost:8000/docs\")\n",
    "print(\"\\nPress the stop button to terminate the server\\n\")\n",
    "\n",
    "!python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo"
   },
   "source": [
    "## Demo UI\n",
    "\n",
    "Launch the Gradio demo interface for interactive testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start-demo"
   },
   "outputs": [],
   "source": [
    "# Start the Gradio demo\n",
    "!pip install -q gradio\n",
    "\n",
    "print(\"Starting Gradio demo...\")\n",
    "print(\"Demo will be available at the URL shown below (usually with a public link)\")\n",
    "print(\"\\nPress the stop button to terminate the demo\\n\")\n",
    "\n",
    "!python demo/app.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tips"
   },
   "source": [
    "## Tips for Google Colab\n",
    "\n",
    "1. **Session Timeout**: Colab sessions disconnect after ~12 hours or inactivity. Save checkpoints to Drive regularly.\n",
    "2. **GPU Memory**: Colab provides ~15-16GB GPU memory (T4) or ~50GB (A100 if available). Use gradient checkpointing if needed.\n",
    "3. **Storage**: Colab provides ~80GB temporary storage. Use Google Drive for persistent storage.\n",
    "4. **Installation**: First run may take 10-15 minutes to install all dependencies.\n",
    "5. **Data Transfer**: Upload large datasets to Google Drive first, then copy to Colab local storage for faster access.\n",
    "6. **Checkpoints**: Always save model checkpoints to Google Drive to avoid losing progress.\n",
    "7. **Free Tier Limits**: Colab has usage limits. Consider Colab Pro for longer sessions.\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **CUDA out of memory**: Reduce batch size or use gradient accumulation\n",
    "- **Import errors**: Ensure you ran `pip install -e .`\n",
    "- **Data not found**: Verify your data directory structure\n",
    "- **Slow training**: Make sure you're using GPU (check nvidia-smi output)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Upload your dataset to `./data/`\n",
    "2. Run data preparation\n",
    "3. Train the crop router\n",
    "4. Train each crop adapter through all 3 phases\n",
    "5. Evaluate and visualize results\n",
    "6. Deploy with the API or demo UI\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}