\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{float}
\usepackage{enumitem}

\geometry{margin=2.5cm}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{\textbf{AADS-ULoRA v5.5 Implementation Guide -- Part 1}\\Environment Setup, Crop Router, Phase 1 Training, Dynamic OOD Configuration}
\author{Agricultural AI Development Team}
\date{March 2026--Version}

\begin{document}

\maketitle

\tableofcontents
\newpage

\section{Critical Changes from v5.4}

AADS-ULoRA v5.5 enhances v5.4's independent architecture with improved OOD detection through dynamic, per-class Mahalanobis thresholds. This eliminates the need for manual threshold tuning while improving detection accuracy across variable disease classes.

\subsection{What Was Enhanced}

\begin{table}[H]
\centering
\caption{v5.4 to v5.5 Enhancements}
\begin{tabular}{lll}
\toprule
\textbf{Component} & \textbf{v5.4} & \textbf{v5.5} \\
\midrule
OOD Thresholds & Fixed global values & Dynamic per-class computation \\
Threshold Computation & Manual tuning & Automatic from validation statistics \\
Detection Sensitivity & Uniform across classes & Adapted to class variability \\
\bottomrule
\end{tabular}
\end{table}

\subsection{What Was Kept}

\begin{itemize}[leftmargin=*]
    \item Simple crop router (replaces SEMA)
    \item Independent crop adapters (no cross-adapter interference)
    \item DoRA for Phase 1 base initialization
    \item SD-LoRA for Phase 2 class-incremental learning
    \item CONEC-LoRA for Phase 3 data-incremental learning
    \item Mahalanobis++ OOD detection framework
\end{itemize}

\section{Environment Setup}

\begin{lstlisting}[language=Python, caption={Environment Setup for v5.5}]
"""
============================================================
AADS-ULoRA v5.5 Independent Multi-Crop - Environment Setup
============================================================
Run this first in Google Colab Pro
"""

import torch
import sys
import os

print("="*60)
print("AADS-ULoRA v5.5 Independent Multi-Crop Setup")
print("="*60)

# Step 1: Verify GPU
print("\n[Step 1] Verifying GPU...")

assert torch.cuda.is_available(), "ERROR: No GPU available!"
gpu_name = torch.cuda.get_device_name(0)
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU: {gpu_name}")

if "A100" not in gpu_name:
    print(f"WARNING: Not an A100! Got {gpu_name}")
    print("Training may be slower or OOM")

total_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
print(f"GPU Memory: {total_mem:.1f} GB")

BATCH_SIZE = 8 if total_mem < 45 else 16

# Step 2: Install Dependencies
print("\n[Step 2] Installing dependencies...")

!pip install -q transformers==4.56.0 --upgrade
!pip install -q peft>=0.8.0 --upgrade
!pip install -q accelerate bitsandbytes
!pip install -q scikit-learn matplotlib seaborn
!pip install -q tqdm pillow opencv-python
!pip install -q albumentations

import transformers
import peft

print(f"Transformers: {transformers.__version__}")
print(f"PEFT: {peft.__version__}")

# Verify PEFT supports DoRA
peft_version = tuple(map(int, peft.__version__.split('.')[:2]))
assert peft_version >= (0, 8), "PEFT must be >= 0.8.0 for DoRA!"
print(f"PEFT {peft.__version__} supports DoRA")

# Step 3: Mount Google Drive
print("\n[Step 3] Mounting Google Drive...")

from google.colab import drive
drive.mount('/content/drive')

# Step 4: Create v5.5 Directory Structure
ROOT = '/content/drive/MyDrive/AADS_v55_Independent'
folders = [
    'data',              # Per-crop data
    'adapters',          # Independent crop adapters
    'router',            # Crop router model
    'checkpoints',       # Training checkpoints
    'prototypes',        # Per-crop prototypes with OOD stats
    'logs',              # Training logs
    'ood_stats'          # Per-class OOD threshold statistics
]

for folder in folders:
    os.makedirs(f"{ROOT}/{folder}", exist_ok=True)
    print(f"Created {ROOT}/{folder}")

print("\nEnvironment setup complete!")
print(f"Batch size: {BATCH_SIZE}")
print(f"Project root: {ROOT}")
\end{lstlisting}

\section{Simple Crop Router Implementation}

Unlike v5.3's SEMA-based expansion, v5.5 uses a simple crop classifier (unchanged from v5.4). This is a much easier task (crop identification vs. disease detection) and achieves 98\%+ accuracy with minimal training.

\subsection{Crop Router Architecture}

\begin{lstlisting}[language=Python, caption={Simple Crop Router Class}]
import torch
import torch.nn as nn
from transformers import AutoModel
from typing import List

class SimpleCropRouter:
    """
    Lightweight crop classifier using DINOv2 linear probe.
    Much simpler than SEMA - just identifies which crop.
    
    Literature: Standard transfer learning
    Target: 98%+ crop classification accuracy
    """
    def __init__(self, crops: List[str], device='cuda'):
        self.crops = crops
        self.device = device
        
        # Load DINOv2 as feature extractor (smaller base model)
        self.backbone = AutoModel.from_pretrained(
            'facebook/dinov2-base'  # 86M params vs 1.1B for giant
        ).to(device)
        self.backbone.eval()
        
        # Freeze backbone
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # Linear classifier for crop types
        self.classifier = nn.Linear(768, len(crops)).to(device)
        
        print(f"Crop router initialized for {len(crops)} crops")
    
    def train(self, crop_dataset, epochs=10, lr=1e-3):
        """
        Train crop classifier on labeled crop images.
        
        Args:
            crop_dataset: DataLoader with (images, crop_labels)
            epochs: Training epochs
            lr: Learning rate
            
        Target: 98% accuracy (easy task)
        """
        optimizer = torch.optim.AdamW(
            self.classifier.parameters(),
            lr=lr
        )
        criterion = nn.CrossEntropyLoss()
        
        best_acc = 0.0
        
        for epoch in range(epochs):
            self.classifier.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for images, labels in crop_dataset:
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # Extract features (frozen backbone)
                with torch.no_grad():
                    features = self.backbone(images).last_hidden_state[:, 0]
                
                # Classify crop
                logits = self.classifier(features)
                loss = criterion(logits, labels)
                
                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # Metrics
                total_loss += loss.item()
                _, predicted = logits.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
            
            acc = 100.0 * correct / total
            avg_loss = total_loss / len(crop_dataset)
            
            print(f"Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Acc={acc:.2f}%")
            
            if acc > best_acc:
                best_acc = acc
                self.save_checkpoint(f'router_epoch{epoch+1}.pth')
        
        print(f"\nBest crop routing accuracy: {best_acc:.2f}%")
        return best_acc
    
    def route(self, image: torch.Tensor) -> str:
        """
        Predict crop type from image.
        
        Args:
            image: Tensor [1, 3, H, W]
            
        Returns:
            crop_name: One of self.crops
        """
        self.classifier.eval()
        
        with torch.no_grad():
            features = self.backbone(image).last_hidden_state[:, 0]
            logits = self.classifier(features)
            crop_idx = logits.argmax(dim=1).item()
        
        return self.crops[crop_idx]
    
    def save_checkpoint(self, path):
        """Save router checkpoint."""
        torch.save({
            'classifier': self.classifier.state_dict(),
            'crops': self.crops
        }, path)
    
    def load_checkpoint(self, path):
        """Load router checkpoint."""
        checkpoint = torch.load(path)
        self.classifier.load_state_dict(checkpoint['classifier'])
        self.crops = checkpoint['crops']
\end{lstlisting}

\subsection{Training the Crop Router}

\begin{lstlisting}[language=Python, caption={Crop Router Training Script}]
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

class CropClassificationDataset(Dataset):
    """
    Dataset for crop type classification.
    Can use PlantCLEF or custom crop images.
    """
    def __init__(self, image_paths, crop_labels, transform=None):
        self.image_paths = image_paths
        self.crop_labels = crop_labels  # Numeric labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image = Image.open(self.image_paths[idx]).convert('RGB')
        image = np.array(image)
        
        if self.transform:
            image = self.transform(image=image)['image']
        
        return image, self.crop_labels[idx]

# Augmentation for crop router
crop_transform = A.Compose([
    A.Resize(224, 224),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.3),
    A.RandomBrightnessContrast(p=0.2),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2()
])

# Create dataset
crops = ['tomato', 'pepper', 'corn']
train_dataset = CropClassificationDataset(
    image_paths=train_image_paths,
    crop_labels=train_crop_labels,  # 0=tomato, 1=pepper, 2=corn
    transform=crop_transform
)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Train router
router = SimpleCropRouter(crops=crops, device='cuda')
router.train(train_loader, epochs=10)

# Validate
val_acc = evaluate_router(router, val_loader)
print(f"Validation accuracy: {val_acc:.2f}%")
assert val_acc >= 95.0, "Crop router accuracy too low!"
\end{lstlisting}

\section{Independent Crop Adapter Architecture}

\begin{lstlisting}[language=Python, caption={Independent Crop Adapter Class with Dynamic OOD}]
from peft import LoraConfig, get_peft_model, TaskType
import torch
import torch.nn as nn
from typing import Dict, List, Optional
import numpy as np
from sklearn.covariance import EmpiricalCovariance

class IndependentCropAdapter:
    """
    Self-contained adapter for one crop with dynamic OOD detection.
    No communication with other crop adapters.
    
    Lifecycle:
    - Phase 1: DoRA base initialization
    - Phase 2: SD-LoRA add new diseases
    - Phase 3: CONEC-LoRA fortify existing diseases
    
    OOD Detection:
    - Dynamic per-class Mahalanobis thresholds
    - Computed from validation data statistics
    """
    def __init__(self, crop_name: str, device='cuda'):
        self.crop_name = crop_name
        self.device = device
        self.phase = 0
        self.disease_classes = []
        
        # Load frozen DINOv2 backbone
        from transformers import AutoModel
        self.backbone = AutoModel.from_pretrained(
            'facebook/dinov2-giant'
        ).to(device)
        
        # Freeze backbone
        for param in self.backbone.parameters():
            param.requires_grad = False
        
        # DoRA adapter (initialized in Phase 1)
        self.adapter = None
        self.classifier = None
        self.prototypes = None
        
        # Dynamic OOD statistics
        self.ood_stats = {
            'class_means': {},      # Per-class Mahalanobis mean
            'class_stds': {},       # Per-class Mahalanobis std
            'threshold_factor': 2.0  # k sigma factor (95% confidence)
        }
        
        print(f"Initialized adapter for {crop_name}")
    
    def phase1_initialize(self, train_data, val_data, config):
        """
        Phase 1: Base initialization with DoRA.
        Computes dynamic OOD thresholds from validation data.
        
        Literature: Liu et al. (2024) - DoRA
        Target: 95%+ clean accuracy
        """
        # DoRA configuration
        lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=config['lora_r'],
            lora_alpha=config['lora_alpha'],
            use_dora=True,  # Enable DoRA decomposition
            target_modules=['query', 'value'],
            lora_dropout=0.1
        )
        
        # Apply DoRA to backbone
        self.adapter = get_peft_model(self.backbone, lora_config)
        
        # Add classifier head
        self.classifier = nn.Linear(
            1536,  # DINOv2-giant output dim
            len(train_data.classes)
        ).to(self.device)
        
        # Train Phase 1
        self._train_phase1(train_data, config)
        
        # Compute Mahalanobis prototypes
        self.prototypes = self._compute_prototypes(train_data)
        
        # Compute dynamic OOD thresholds from validation data
        self._compute_dynamic_ood_thresholds(val_data)
        
        self.disease_classes = train_data.classes
        self.phase = 1
        
        print(f"\n{self.crop_name} Phase 1 complete")
        print(f"Disease classes: {self.disease_classes}")
        print(f"OOD thresholds computed for {len(self.disease_classes)} classes")
    
    def _compute_dynamic_ood_thresholds(self, val_data):
        """
        Compute per-class Mahalanobis distance statistics on validation data.
        These determine dynamic OOD thresholds.
        """
        self.adapter.eval()
        self.classifier.eval()
        
        # Collect features and predictions per class
        class_distances = {cls: [] for cls in range(len(self.disease_classes))}
        
        with torch.no_grad():
            for images, labels in val_data:
                images = images.to(self.device)
                
                # Forward pass
                features = self.adapter(images).last_hidden_state[:, 0]
                logits = self.classifier(features)
                predictions = logits.argmax(dim=1)
                
                # Compute Mahalanobis distance for each sample
                for i, (feat, pred) in enumerate(zip(features, predictions)):
                    # Distance to predicted class prototype
                    cls = pred.item()
                    mean = self.prototypes['means'][cls]
                    cov = self.prototypes['covariances'][cls]
                    
                    diff = feat.cpu() - mean
                    # Mahalanobis distance: sqrt(diff^T * inv(cov) * diff)
                    try:
                        cov_inv = torch.inverse(cov.cpu())
                        dist = torch.sqrt(diff @ cov_inv @ diff.T).item()
                        class_distances[cls].append(dist)
                    except:
                        # Fallback if covariance is singular
                        dist = torch.norm(diff).item()
                        class_distances[cls].append(dist)
        
        # Compute statistics per class
        for cls in range(len(self.disease_classes)):
            if len(class_distances[cls]) > 0:
                distances = np.array(class_distances[cls])
                self.ood_stats['class_means'][cls] = float(np.mean(distances))
                self.ood_stats['class_stds'][cls] = float(np.std(distances))
            else:
                # Fallback if no samples predicted as this class
                self.ood_stats['class_means'][cls] = 0.0
                self.ood_stats['class_stds'][cls] = 1.0
        
        # Save OOD statistics
        self._save_ood_stats()
        
        print(f"Dynamic OOD thresholds computed:")
        for cls, name in enumerate(self.disease_classes):
            mean = self.ood_stats['class_means'][cls]
            std = self.ood_stats['class_stds'][cls]
            threshold = mean + self.ood_stats['threshold_factor'] * std
            print(f"  {name}: mean={mean:.2f}, std={std:.2f}, threshold={threshold:.2f}")
    
    def get_ood_threshold(self, class_idx: int) -> float:
        """Get dynamic OOD threshold for specific class."""
        mean = self.ood_stats['class_means'].get(class_idx, 0.0)
        std = self.ood_stats['class_stds'].get(class_idx, 1.0)
        return mean + self.ood_stats['threshold_factor'] * std
    
    def _save_ood_stats(self, path=None):
        """Save OOD statistics to disk."""
        if path is None:
            path = f"./ood_stats/{self.crop_name}_ood_stats.pt"
        torch.save(self.ood_stats, path)
    
    def load_ood_stats(self, path):
        """Load OOD statistics from disk."""
        self.ood_stats = torch.load(path)
\end{lstlisting}

\section{Phase 1 Training Details}

\begin{lstlisting}[language=Python, caption={Phase 1 DoRA Training}]
def _train_phase1(self, train_data, config):
    """
    DoRA training with LoRA+ optimizer.
    
    LoRA+: Use 16x learning rate for B matrices
    Target: 95% clean accuracy
    """
    # LoRA+ optimizer configuration
    optimizer_params = [
        {
            'params': [p for n, p in self.adapter.named_parameters() 
                      if 'lora_B' in n],
            'lr': config['base_lr'] * config['loraplus_lr_ratio']  # 16x
        },
        {
            'params': [p for n, p in self.adapter.named_parameters() 
                      if 'lora_A' in n or 'lora_magnitude' in n],
            'lr': config['base_lr']
        },
        {
            'params': self.classifier.parameters(),
            'lr': config['base_lr']
        }
    ]
    
    optimizer = torch.optim.AdamW(optimizer_params, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    best_acc = 0.0
    
    for epoch in range(config['phase1_epochs']):
        self.adapter.train()
        self.classifier.train()
        
        epoch_loss = 0
        correct = 0
        total = 0
        
        for images, labels in train_data:
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # Forward pass
            features = self.adapter(images).last_hidden_state[:, 0]
            logits = self.classifier(features)
            loss = criterion(logits, labels)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # Metrics
            epoch_loss += loss.item()
            _, predicted = logits.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        # Evaluate
        val_acc = self._evaluate(train_data.val_loader)
        avg_loss = epoch_loss / len(train_data)
        
        print(f"Epoch {epoch+1}/{config['phase1_epochs']}: "
              f"Loss={avg_loss:.4f}, Val Acc={val_acc:.2f}%")
        
        # Save best
        if val_acc > best_acc:
            best_acc = val_acc
            self._save_checkpoint('phase1_best.pth')
    
    # Load best checkpoint
    self._load_checkpoint('phase1_best.pth')
    print(f"\nPhase 1 best accuracy: {best_acc:.2f}%")
    
    assert best_acc >= 0.95, f"Phase 1 accuracy {best_acc:.2f}% < 95%!"
\end{lstlisting}

\section{Mahalanobis Prototype and Dynamic OOD Computation}

\begin{lstlisting}[language=Python, caption={Prototype and Dynamic OOD Computation}]
def _compute_prototypes(self, train_data):
    """
    Compute class-conditional Mahalanobis prototypes.
    
    Returns:
        prototypes: Dict with 'means', 'covariances', and 'num_classes'
    """
    from sklearn.covariance import EmpiricalCovariance
    
    self.adapter.eval()
    
    # Collect features per class
    class_features = {cls: [] for cls in range(len(self.disease_classes))}
    
    with torch.no_grad():
        for images, labels in train_data:
            images = images.to(self.device)
            
            features = self.adapter(images).last_hidden_state[:, 0]
            
            for feat, label in zip(features.cpu().numpy(), labels.numpy()):
                class_features[label].append(feat)
    
    # Compute means and covariances
    means = {}
    covariances = {}
    
    for cls in range(len(self.disease_classes)):
        features = np.array(class_features[cls])
        
        # Mean
        means[cls] = torch.tensor(features.mean(axis=0)).to(self.device)
        
        # Covariance with regularization for numerical stability
        cov_estimator = EmpiricalCovariance().fit(features)
        cov_matrix = torch.tensor(cov_estimator.covariance_).to(self.device)
        
        # Add small regularization to prevent singularity
        cov_matrix += torch.eye(cov_matrix.shape[0], device=self.device) * 1e-4
        covariances[cls] = cov_matrix
    
    return {
        'means': means,
        'covariances': covariances,
        'num_classes': len(self.disease_classes)
    }

def compute_mahalanobis_distance(self, features, class_idx=None):
    """
    Compute minimum Mahalanobis distance to class prototypes.
    If class_idx specified, compute distance to that class only.
    
    Returns:
        distance: Mahalanobis distance
        predicted_class: Index of nearest class (if class_idx is None)
    """
    min_distance = float('inf')
    predicted_class = class_idx if class_idx is not None else 0
    
    classes_to_check = [class_idx] if class_idx is not None else range(self.prototypes['num_classes'])
    
    for cls in classes_to_check:
        mean = self.prototypes['means'][cls]
        cov = self.prototypes['covariances'][cls]
        
        diff = features - mean
        
        try:
            cov_inv = torch.inverse(cov)
            distance = torch.sqrt(diff @ cov_inv @ diff.T).item()
        except:
            distance = torch.norm(diff).item()
        
        if class_idx is None and distance < min_distance:
            min_distance = distance
            predicted_class = cls
        elif class_idx is not None:
            min_distance = distance
    
    return min_distance, predicted_class

def detect_ood_dynamic(self, image):
    """
    OOD detection using dynamic per-class thresholds.
    
    Returns:
        result: Dict with 'is_ood', 'predicted_class', 'confidence', 
                'mahalanobis_distance', 'threshold'
    """
    self.adapter.eval()
    self.classifier.eval()
    
    with torch.no_grad():
        features = self.adapter(image).last_hidden_state[:, 0]
        logits = self.classifier(features)
        
        # Get prediction
        probs = torch.softmax(logits, dim=1)
        confidence, predicted_class = probs.max(1)
        predicted_class = predicted_class.item()
        confidence = confidence.item()
        
        # Compute Mahalanobis distance to predicted class
        maha_dist, _ = self.compute_mahalanobis_distance(
            features[0], class_idx=predicted_class
        )
        
        # Get dynamic threshold for predicted class
        threshold = self.get_ood_threshold(predicted_class)
        
        # OOD decision
        is_ood = maha_dist > threshold
    
    return {
        'is_ood': is_ood,
        'predicted_class': predicted_class,
        'disease_name': self.disease_classes[predicted_class],
        'confidence': confidence,
        'mahalanobis_distance': maha_dist,
        'threshold': threshold,
        'ood_score': maha_dist / threshold if threshold > 0 else maha_dist
    }
\end{lstlisting}

\section{Summary}

Part 1 covered:
\begin{itemize}[leftmargin=*]
    \item Environment setup for v5.5
    \item Simple crop router implementation (replaces SEMA)
    \item Independent crop adapter architecture with dynamic OOD
    \item Phase 1 DoRA training with LoRA+ optimizer
    \item Mahalanobis prototype computation
    \item Dynamic per-class OOD threshold computation from validation data
\end{itemize}

Part 2 will cover:
\begin{itemize}[leftmargin=*]
    \item Phase 2: SD-LoRA for class-incremental learning
    \item Phase 3: CONEC-LoRA for data-incremental learning
    \item Complete multi-crop pipeline integration with dynamic OOD
    \item Gradio demonstration interface
\end{itemize}

\end{document}